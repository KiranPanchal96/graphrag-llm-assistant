# ğŸ“š graphrag-llm-assistant

A side project implementing **RAG (Retrieval-Augmented Generation)** and **GraphRAG** pipelines for answering questions using both **vector search** (FAISS + embeddings) and **graph-based reasoning** (NetworkX / Neo4j).

The project integrates **LangChain**, **FastAPI**, **Gradio UI**, and **Langfuse** for prompt management and evaluation.

---

## ğŸš€ Features

- **RAG pipeline** using FAISS vector store & HuggingFace embeddings  
- **GraphRAG pipeline** using NetworkX / Neo4j graph reasoning  
- **LLM-based evaluation** (QAEvalChain style) + BLEU, ROUGE-L, BERTScore  
- **Prompt management** with Langfuse  
- **Multiple LLM options** â€” OpenAI API or local Ollama  
- **FastAPI backend** for programmatic access  
- **Gradio UI** for interactive queries  
- **Dockerized deployment**  
- **Pre-commit hooks** for linting, formatting, and type checking  

---

## ğŸ› ï¸ Tech Stack

- **LangChain** â€“ Orchestration of retrieval + generation  
- **HuggingFace** â€“ Embedding models  
- **FAISS** â€“ Vector store for semantic search  
- **NetworkX / Neo4j** â€“ Graph reasoning  
- **FastAPI** â€“ REST API backend  
- **Gradio** â€“ Interactive web UI  
- **Langfuse** â€“ Prompt and evaluation tracking  
- **Ollama** â€“ Local LLM hosting (optional)  
- **Docker** â€“ Containerised deployment  
- **pre-commit** â€“ Linting, formatting, type checking  

---

## ğŸ“‚ Folder Structure
graphrag-llm-assistant/
â”‚
â”œâ”€â”€ data/ # Prompt examples, evaluation data (stripped in public repo)
â”œâ”€â”€ embeddings/ # FAISS vector store and embedding model files
â”œâ”€â”€ outputs/ # Generated evaluation results and logs
â”‚
â”œâ”€â”€ src/ # Source code
â”‚ â”œâ”€â”€ api/ # FastAPI backend & Gradio UI
â”‚ â”œâ”€â”€ evaluators/ # Evaluation scripts for RAG & GraphRAG
â”‚ â”œâ”€â”€ inference/ # RAG & GraphRAG pipelines
â”‚ â”œâ”€â”€ prompts/ # Prompt templates & loaders
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ tests/ # Unit and integration tests
â”‚
â”œâ”€â”€ .pre-commit-config.yaml # Pre-commit hooks configuration
â”œâ”€â”€ docker-compose.yml # Docker Compose setup
â”œâ”€â”€ Dockerfile # Docker image build file
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Project documentation
â”œâ”€â”€ LICENSE # License file (MIT)
â””â”€â”€ .gitignore # Git ignore rules

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository
git clone https://github.com/KiranPanchal96/graphrag-llm-assistant.git

cd graphrag-llm-assistant

### 2ï¸âƒ£ Create & Activate a Virtual Environment
python3 -m venv .venv
source .venv/bin/activate   # macOS/Linux
.venv\Scripts\activate      # Windows

### 3ï¸âƒ£ Install Dependencies
pip install --upgrade pip
pip install -r requirements.txt

---

## âš™ï¸ Environment Variables

Create a .env file in the project root with the following variables:

<pre>
  OPENAI_API_KEY=your_openai_api_key
  LANGFUSE_PUBLIC_API_KEY=your_langfuse_public_key
  LANGFUSE_SECRET_API_KEY=your_langfuse_secret_key
  LANGFUSE_HOST=https://cloud.langfuse.com
  FASTAPI_HOST=localhost
  FASTAPI_PORT=8000
  USE_OLLAMA=0
  OLLAMA_MODEL=llama3 
</pre>

---

## ğŸƒ Running the Project
### 1ï¸âƒ£ Start the FastAPI Backend
uvicorn src.api.s09a_fastapi:app --reload --host 0.0.0.0 --port 8000

### 2ï¸âƒ£ Launch the Gradio UI
python src/api/s10a_gradio_ui.py
Then visit http://localhost:7860 in your browser.

---

## ğŸ“Š Evaluation
Run evaluation scripts to assess model performance:

RAG Evaluation:
python src/evaluators/s08a_evaluator.py

GraphRAG Evaluation:
python src/evaluators/s08b_nwkx_graph_evaluator.py

Results are saved in outputs/eval_results/ as timestamped JSON files.

---

## ğŸ³ Docker Deployment
Build the image:
docker build -t graphrag-assistant .

Run with Docker Compose:
docker-compose up --build

This will start both the FastAPI backend and Gradio UI.

---

## ğŸ§ª Running Tests
pytest tests/ --maxfail=1 --disable-warnings -q

---

## ğŸ“œ License
This project is licensed under the MIT License. See LICENSE for details.
